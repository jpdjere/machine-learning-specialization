# State-action value function

## State-action value function definition

when we start to develop reinforcement learning hours later this week, you see that there's a key quantity that reinforcement learning algorithm will try to compute and that's called the state action value function. Let's take a look at what this function is. The state action value function is a function typically denoted by the letter uppercase Q. And it's a function of a state you might be in as well as the action you might choose to take in that state and Q of s,a. Will give a number that equals the return. If you start in that state. S and take the action A just once and after taking action A once you then behave optimally after that. So after that you take whatever actions will result in the highest possible return. Now you might be thinking there's something a little bit strange about this definition because how do we know what is the optimal behavior? And if we knew what the optimal behavior, if we already knew what's the best action to take in every state, why do we still need to compute Q of SA. Because we already have the optimal policy. So I do want to acknowledge that there's something a little bit strange about this definition. There's almost something a little bit circular about this definition, but rest assured When we look at specific reinforcement learning algorithms later will resolve this slightly circular definition and will come up with a way to compute the Q function even before we've come up with the optimal policy. But you see that in a later video. So don't worry about this for now. Let's look at an example we saw previously that this is a pretty good policy Go left from stage 2, 3 and four and go right from State five. It turns out that this is actually the optimal policy for the mars rover application When the discount factor gamma is 0.5, so Q of S. A will be equal to the total return If you start from say that take the action A and then behave optimally after that. Meaning take actions according to this policy. Shown over here, let's figure out what Q of s,a. Is for a few different states. Let's look at say Q of state too. And what if we take the action to go right well if you're in state two and you go right then you end up at state three And then after that you behave optimally you're going to go left from ST three and then go left from state to and then eventually you get the reward of 100. In this case, the rewards you get would be zero from state to zero when you get to stay three zero when you get back to state two and then 100 when you finally get to the terminal state one and so the return will be zero plus 0.5 times that plus 0.5 squared times ac plus 0.5 cubed times 100. And this turns out to be 12.5 And so Q of ST two of going right as equal to 12.5. Note that this passes, no judgment on whether going right is a good idea or not. It's actually not that good an idea from state two to go right, but it just faithfully reports out the return if you take action A and then behave optimally afterwards. Here's another example. If you're in state to and you were to go left, then the sequence of rewards you get will be zero when you're in state two followed by 100. And so the return is zero plus 0.5 times 100 that's equal to 50 in order to write down The values of Q(s,a). In this diagram, I'm going to write 12.5 here on the right to denote that this is Q of state two going to the right. And then when I write a little 50 here on the left to denote that this is Q of state two and going to the left just to take one more example What if we're in state four and we decide to go left. Well if you're in state four you go left, you get rewards zero and then you take action left here. So zero gain, take action left here, zero and then 100. So Q of four Left results in rewards zero because the first action is left and then because we followed the optimal policy afterwards You can reward 00 100. And so the return is zero plus 00.5 times that. Plus 4.5 squared times that plus 0.5 cubed times that. Which is therefore equal to 12.5. So Q4 left is 12.5. I'm going to write this here as 12.5. And it turns out if you were to carry out this exercise for all of the other states and all of the other actions, you end up with this being the Q of s,a. For different states and different actions And then finally at the Terminal State. Well it doesn't matter what you do, you just get that terminal reward 100 or 40. So just write down those terminal awards over here. So this is Q of s,a. For every state state one through six and for the two actions, action left and action right. Because the state action value function is almost always denoted by the letter Q. This is also often called the Q function. So the terms Q. Function and state action value function are used interchangeably and it tells you what are your returns or really what is the value? How good is it? Just take action A and state S and then behave optimally after that. Now it turns out that once you can compute the Q function this will give you a way to pick actions as well. Here's the policy and return. And here are the values Q of s,a. From the previous slide. You notice one interesting thing when you look at the different states which is that if you take state two taking the action left results in a,q. Value or state action value of 50 which is actually the best possible return you can get from that state. In state three two of s,a. for the action left also gives you that higher return in state four the action left gives you the return you want. And in state five is actually the action going to the right that gives you that higher return of 20. So it turns out that the best possible return from any state S. Is the largest value of Q of s,a A maximizing over A. Just to make sure this is clear what I'm saying is that in say state for There is Q of state four left which is 12.5 And q. of state four right, Which turns out to be 10. And the larger of these two values which is 12.5 Is the best possible return from that state four. In other words the highest return you can hope to get from State four is 12.5. And it's actually the larger of these two numbers 12.5 and 10. And moreover, if you want your Mars Rover to enjoy a return of 12.5 rather than say 10 then the action you should take is the action A. That gives you the larger value of Q of s,a. So the best possible action status is the action A. That actually maximizes Q, of s,a. So this might give you a hint for why computing Q, of s,a. Is an important part of the reinforcement learning algorithm that will build later. Namely if you have a way of computing Q of s,a. For every state and for every action then when you're in some state s all you have to do is look at the different actions A. And pick the action A. That maximizes Q of s,a. And so pi of s. S can just pick the action A. That gives the largest value of Q of s,a. And that will turn out to be a good action. In fact it turned out to be the optimal action. Another intuition about why this makes sense is Qof s,a. Is returned if you start in the state S and take the action A. And then behave optimally after that. So in order to earn the biggest possible return, what you really want is to take the action A. That results in the biggest total return. That's why if only we have a way of computing Q f s,a. For every state taking the action A that maximizes return under these circumstances seems like it's the best action to take in that state. Although this isn't something you need to know for this course, I want to mention also that if you look online or look at the reinforcement learning literature, sometimes you also see this Q function written as Q. Star instead of Q. And this Q function is sometimes also called the optimal Q function. These terms just refer to the Q function exactly as we've defined it. So if you look at the reinforcement learning literature and read about Q. Star or the Q function, that just means the state action value function that we've been talking about. But for the purposes of this course you don't need to worry about this. So to summarize if you can compute Q of s,a. For every state and every action, then that gives us a good way to compute the optimal policy pi of S. So that's the state action value function or the Q function. We'll talk later about how to come up with an algorithm to compute them despite the slightly circular aspect of the definition of the Q function. But first let's take a look at the next video at some specific examples of what these values Q of s,a. Actually look like

## State-action value function example

Using the state-action value function example. You're seeing what the values of QSA are like. In order to keep holding our intuition about reinforcement learning problems and how the values of QSA change depending on the problem will provided an optional lab. That lets you play around modify the mars rover example and see for yourself how QSA will change. Let's take a look. Here's a Jupyter Notebook that I hope you play with after watching this video. I'm going to run these helper functions, now notice here that this specifies the number six that the two actions so I wouldn't change these. And this specifies the terminal left in the terminal right rewards which has been 100 and 40 and then zero was the rewards of the intermediate states. The discount factor gamma 0.5. And let's ignore the misstep probability for now we'll talk about that in a later video. And with these values if you run this code this will compute and visualize the optimal policy as well as the Q function Q of SA. You learn later about how to develop a learning algorithm to estimate or compute Q of SA yourself. So for now don't worry about what code we have written to compute Q of SA. But you see that the values here Q of SA are the values we saw in the lecture. Now here's where the fun starts. Let's change around some of the values and see how these things change. I'm going to update the terminal right reward to a much smaller value says only 10. If I now rerun the code, look at how Q of SA changes and now thinks that if you're in state 5. Then if you go left and behave optimally you get 6.25. Whereas if you go right and behave also the after that you get a return of only five. So now when the reward at the right is so small it's only 10. Even when you're so close to you rather go left all the way. And in fact the auto policy is now to go left from every single state. Let's make some other changes. I'm going to change the terminal right reward back to 40. But let me change the discount factor to 0.9, with a discount factor that's closer to one. This makes the Mars Rover less impatient is willing to take longer to hold out for a higher reward because rewards in the future are not multiplied by 0.5 to some high power is multiplied by 0.9 to some high power. And so is willing to be more patients, because rewards in the future are not discounted or multiplied by as small a number as when the discount was 0.5. So let's rerun the code. And now you see this is Q of SA for the different states and now for state 5 going left actually gives you a higher reward of 65.61 compared to 36. Notice by the way that 36 is 0.9 times this terminal reward of 40. So these numbers make sense. But when a small patient is willing to go to the left, even when you're in state 5. Now let's change gamma to a much smaller number like 0 .3. So this very heavily discounts rewards in the future. This makes it incredibly impatient. So let me rerun this code and now the behavior has changed. Noticed that now in state 4 is not going to have the patience to go for the larger 100 reward, because the discount factor gamma is now so small is 0.3. It would rather go for the reward of 40 even though it's a much smaller reward is closer and that's whether we choose to do. So I hope that you can get a sense by playing around with these numbers yourself and running this code. How the values of Q of SA change as was how the optimal return which you notice is the larger of these two numbers QSA. How that changes as well as how the optimal policy also changes. So I hope you go and play with the optional lab and change the reward function and change the discount factor gamma and try a different values. And see for yourself how the values of Q of SA change, how the optimal return from different states change and how the auto policy changes depending on these different values. And by doing so, I hope that will sharpen your intuition about how these different quantities are affected depending on the rewards and so on in reinforcement learning application. After you play to the lab, we then be ready to come back and talk about what's probably the single most important equation in reinforcement learning, which is something called the bellman equation. So I hope you have fun playing with the optional lab and after that let's come back to talk about bellman equations.

## Bellman Equation

Let me summarize where we are. If you can compute the state action value function Q of S, A, then it gives you a way to pick a good action from every scene. Just pick the action A, that gives you the largest value of Q of S,A. The question is, how do you compute these values Q of S,A? In reinforcement learning, there's a key equation called the Bellman equation that will help us to compute the state action value function. Let's take a look at what is this equation. As a reminder, this is the definition of Q of S, A. It's return if we start in state S, take the action a once and they behave optimally after that. In order to describe the Bellman equation, I'm going to use the following notation. I'm going to use S to denote the current state. Next, I'm going to use R of S to denote the rewards of the current state. For our little MDP example, we will have that r of one State1 is 100. The reward of State 2 is 0, and so on. The reward of State 6 is 40. I'm going to use the alphabet A to denote the current action, the action that you take in the state S. After you take the action a, you get to some new state. For example, if you're in State 4 and you take the action left, then you get to State 3. I'm going to use S prime to denote the state you get to after taking that action a from the current state S. I'm also going to use A prime to denote the action that you might take in state S prime, the new state you got to. The notation convention, by the way, is that S,A correspond to the current state and action. When we add the prime, that's the next state, then the next action. The Bellman equation is the following. It says that Q of S,A, that is the return under this set of assumptions that's equal to r of S, says reward you get for being in that state plus the discount factor Gamma times max over all possible actions, a prime of q of S prime, the new state you just got to, and then a prime. There's a lot going on in this equation. Let's first take a look at some examples. We'll come back to see why this equation might make sense. Let's look at an example. Let's look at Q of State 2 and action. Apply Bellman Equation to this to see what value it gives us. If the current state is state two and that the action is to go right, then the next day you get to, after going write S prime would be the State 3. The Bellman equation says Q of 2, right is R of S. This R State 2, which is just the rewards zero plus the discount factor Gamma, which we set to 0.5 in this example, times max of the Q values in state S prime in State 3. This is going to be the max of 25 and 6.25, since this is max over a prime of q of S prime comma a prime. This is taking the larger of 25 or 6.25 because those are the two choices for State 3. This turns out to be equal to zero plus 0.5 times 25, which is equal to 12.5, which fortunately is Q of two and then the action right. Let's look at just one more example. Let me take the State 4 and see what is Q of State 4 if you decide to go left. In this case, the current state is four current action is to go left. The next state, if you can start from four going left. You end up also at State 3. Let us prime this three again, the Bellman Equation, we'll say this is equal to R of S. Our State four, which is zero plus 0.5 the discount factor Gamma of max over a prime of q of S prime. That is the State 3 again, comma a prime. Once again, the Q values for State 3 are 25 and 6.25 and the larger of these is 25. This works out to be R(4) is 0 plus 0.5 times 25, which is again equal to 12.5. That's why q of four with the action left is also equal to 12.5, just one note, if you're in a terminal state, then Bellman Equation simplifies to q of SA equals to r of S because there's no state S prime and so that second term would go away. Which is why Q of S,A in the terminal states is just 100, 100 or 40 40. If you wish feel free to pause the video and apply the Bellman Equation to any other state action in this MDP and check for yourself if this math works out. Just to recap, this is how we had define Q of S,A. We saw earlier that the best possible return from any state S is max over a Q of S,A. In fact, just to rename SNA, it turns out that the best possible return from a state S prime, is max over S prime of a prime. I didn't really do anything other than rename S, S prime and a to a prime. But this will make some of the intuitions a little bit easier later. But for any state S prime, like State 3, the best possible return from, say, State 3 is the max over all possible actions of Q of S prime a prime. Here again is the Bellman equation. The intuition that this captures is if you're starting from state s and you're going to take action a and then act optimally after that, then you're going to see some sequence of rewards over time. In particular, the return will be computed from the reward at the first step, plus Gamma times reward at the second step plus Gamma squared times reward at the third step, and so on. Plus dot, dot, dot until you get to the terminal state. What Bellman equation says is this sequence of rewards, what the discount factor is, can be broken down into two components. First, this R of s, that's the reward you get right away. In the reinforcement learning literature, this is sometimes also called the immediate reward, but that's what R_1 is. It's the reward you get for starting out in some state s. The second term then is the following; after you start in state s and take action a, you get to some new state s prime. The definition of Q of s, a assumes we're going to behave optimally after that. After we get to s prime, we are going to behave optimally and get the best possible return from the state s prime. What this is, max of a prime of Q of s prime a prime, this is the return from behaving optimally, starting from the state s prime. That's exactly what we had written up here, is the best possible return for when you start from state s prime. Another way of phrasing this is this total return down here is also equal to R_1 plus, and then we're going to factor out Gamma in the map, is Gamma times R_2 plus, and instead of Gamma squared is just Gamma times R_3 plus Gamma squared times R_4 plus dot dot dot. Notice that if you were starting from state s prime, the sequence of rewards you get will be R_2, R_3, then R_4, and so on. That's why this expression here, that's the total return if you were to start from state s prime. If you were to behave optimally, then this expression should be the best possible return for starting from state s prime, which is why this sequence of discount rewards equals that max of a prime of Q of s prime a prime and there were also leftover with this extra discount factor Gamma there, which is why Q of s, a is also equal to this expression over here. In case you think this is quite complicated and you aren't following all the details, don't worry about it. So long as you apply this equation, you will manage to get the right results. But the high level intuition I hope you take away is that the total return you get in the reinforcement learning problem has two parts. The first part is this reward that you get right away, and then the second part is Gamma times the return you get starting from the next state s prime. As these two components together, R of s plus Gamma times the return from the next state, that is equal to the total return from the current state s. That is the essence of the Bellman equation. Just to relate this back to our earlier example, Q of 4, left. That's the total return for starting State 4 and going left. If you were to go left in State 4 the rewards you get are 0 in State 4, 0 in State 3, 0 in State 2, and then 100, which is why the total return is this; 0.5 squared plus 0.5 cubed, which was 12.5. What Bellman equation is saying is that we can break this up into two pieces. There is this zero, which is R of the state four, and then plus 0.5 times this other sequence, 0 plus 0.50 plus 0.5 squared times 100. But if you look at what this sequence is, this is really the optimal return from the next state s prime that you got to after taking the action left from state four. That's why this is equal to the reward 4 plus 0.5 times the optimal return from State 3. Because if you were to start from State 3, the rewards you get would be zero followed by zero followed by 100, so this is optimal return from State 3 and that's why this is just R of 4 plus 0.5 max over a prime Q of State 3, a prime. I know the Bellman equation, this is somewhat complicated equation breaking down your total returns into the reward you're getting right away. The immediate reward plus Gamma times the returns from the next state s prime. If it makes sense to you, but not fully, it's okay. Don't worry about it. You can still apply Bellman's equations to get a reinforcement learning algorithm to work correctly, but I hope that at least the high level intuition of why breaking down the rewards into what you get right away plus what you get in the future. I hope that makes sense. Before moving on to develop a reinforcement learning algorithm, we have coming up next an optional video on Stochastic Markov decision processes or on reinforcement learning applications where the actions, if you take, can have a slightly random effect. Take look at the optional video if you wish. Then after that, we'll start to develop a reinforcement learning algorithm.

## Random (stochastic) environment

In some applications, when you take an action, the outcome is not always completely reliable. For example, if you command your Mars rover to go left maybe there's a little bit of a rock slide, or maybe the floor is really slippery and so it slips and goes in the wrong direction. In practice, many robots don't always manage to do exactly what you tell them because of wind blowing and off course and the wheel slipping or something else. There's a generalization of the reinforcement learning framework we've talked about so far, which models random or stochastic environments. In this optional video, we'll talk about how these reinforcement learning problems work, continuing with our simplifying Mars Rover example, let's say you take the action and command it to go left. Most of the time you'll succeed but what if 10 percent of the time or 0.1 of the time, it actually ends up accidentally slipping and going in the opposite direction? If you command it to go left, it has a 90 percent chance or 0.9 chance of correctly going in the left direction. But the 0.1 chance of actually heading to the right so that it has a 90 percent chance of ending up in state three in this example and a 10 percent chance of ending up in state five. Conversely, if you were to command it to go right and take the action, right, it has a 0.9 chance of ending up in state five and 0.1 chance of ending up in state three. This would be an example of a stochastic environment. Let's see what happens in this reinforcement learning problem. Let's say you use this policy shown here, where you go left in states 2 3 4 and go rights or try to go right in state five. If you were to start in state four and you were to follow this policy, then the actual sequence of states you visit may be random. For example, in state four, you will go left, and maybe you're a little bit lucky, and it actually gets the state three, and then you try to go left again, and maybe it actually gets there. You tell it to go left again, and it gets to that state. If this is what happens, you end up with the sequence of rewards 000100. But if you were to try this exact same policy a second time, maybe you're a little less lucky, the second time you start here. Try to go left and say it succeeds so a zero from state four zero from state three, here you tell it to go left, but you've got unlucky this time and the robot slips and ends up heading back to state four instead. Then you tell it to go left, and left, and left, and eventually get to that reward of 100. In that case, this will be the sequence of rewards you observe. This one from four to three to four three two then one, or is even possible, if you tell from state four to go left following the policy you may get unlucky even on the first step and you end up going to state five because it slipped. Then state five, you command it to go right, and it succeeds as you end up here. In this case, the sequence of rewards you see will be 0040, because it went from four to five, and then states six, we had previously written out the return as this sum of discounted rewards. But when the reinforcement learning problem is stochastic, there isn't one sequence of rewards that you see for sure instead you see this sequence of different rewards. In a stochastic reinforcement learning problem, what we're interested in is not maximizing the return because that's a random number. What we're interested in is maximizing the average value of the sum of discounted rewards. By average value, I mean if you were to take your policy and try it out a thousand times or a 100,000 times or a million times, you get lots of different reward sequences like that and if you were to take the average over all of these different sequences of the sum of discounted rewards, then that's what we call the expected return. In statistics, the term expected is just another way of saying average. But what this means is we want to maximize what we expect to get on average in terms of the sum of discounted rewards. The mathematical notation for this is to write this as E. E stands for expected value of R1 plus Gamma R2 plus, and so on. The job of reinforcement learning algorithm is to choose a policy Pi to maximize the average or the expected sum of discounted rewards. To summarize, when you have a stochastic reinforcement learning problem or a stochastic Markov decision process the goal is to choose a policy to tell us what action to take in state S so as to maximize the expected return. The last way that this changes, what we've talked about is it modifies Bellman equation a little bit. Here's the Bellman equation exactly as we've written down. But the difference now is that when you take the action a in state s, the next state s prime you get to is random. When you're in state 3 and you tell it to go left the next state s prime it could be the state 2, or it could be the state 4. S prime is now random, which is why we also put an average operator or unexpected operator here. We say that the total return from state s, taking action a, once in a behaving optimally, is equal to the reward you get right away, also called the immediate reward plus the discount factor, Gamma plus what you expect to get on average of the future returns. If you want to sharpen your intuition about what happens with these stochastic reinforcement learning problems. You'd go back to the optional lab that I had shown you just now, where this parameter misstep problem is the probability of your Mars Rover going in the opposite direction, than you had commanded it to. If we said misstep prop two is 0.1 and re-execute the Notebook and so these numbers up here are the optimal return if you were to take the best possible actions, take this optimal policy but the robot were to step in the wrong direction 10 percent of the time and these are the q values for this stochastic NTP. Notice that these values are now a little bit lower because you can't control the robot as well as before. The q values, as well as the optimal returns, have gone down a bit. In fact, if you were to increase the misstep probability, say 40 percent of the time the robot doesn't even go into directions. You're commanding it to only 60 percent of the time. It goes where you told it to, then these values end up even lower because your degree of control over the robot has decreased. I encourage you to play with the optional lab and change the value of the misstep probability and see how that affects the for return or the auto expected return, as well as the Q values, q of s a. Now, in everything we've done so far, we've been using this Markov decision process, this Mars rover with just six states. For many practical applications, the number of states will be much larger. In the next video, we'll take the reinforcement learning or Markov decision process framework we've talked about so far and generalize it to this much richer and maybe even more interesting set of problems with much larger and in particular with continuous state spaces, let's take a look at that in the next video.