# Anomaly Detection

## Finding unusual events

Let's look at our second unsupervised learning algorithm: **Anomaly detection algorithms look at an unlabeled dataset of normal events and learns to detect or to raise a red flag if there is an unusual or an anomalous event.** 

People have used anomaly detection to detect possible problems with aircraft engines that were being manufactured, to check if after it was manufactured it seemed anomalous or if there seemed to be anything wrong with it. 

So, after an aircraft engine rolls off the assembly line, we measure a number of different features of the aircraft engine. So, say feature $x_1$ measures the heat generated by the engine, and feature $x_2$ measures the vibration intensity.

Since arcraft engine manufacturers don't make that many bad engines, the easier type of data to collect is: if we have manufactured $m$ aircraft engines, the features $x_1$ and $x_2$ about how these $m$ engines behave. Probably most of them are normal engines rather than ones with a defect or flaw in them. 

![](2024-03-04-22-30-41.png)

The anomaly detection problem is: **after the learning algorithm has seen these $m$ examples of how aircraft engines typically behave** in terms of how much heat is generated and how much they vibrate, if a brand new aircraft engine were to roll off the assembly line and **it had a new feature vector given by $x_{test}$, we'd like to know: _does this engine look similar to ones that have been manufactured before?_** 

Here's how an anomaly detection algorithm works. Let's plot the examples $x_1$ through $x_m$, and marke two new engines - an OK engine, and an anomaly:

![](2024-03-04-22-33-39.png)


How can we have an algorithm address this problem? The most common way to carry out **anomaly detection is through a technique called density estimation.** When we're given our training sets of $m$ examples, the first thing we do is **build a model for the probability of $x$**. In other words, **the learning algorithm will try to figure out what are the values of the features $x_1$ and $x_2$ that have high probability and what are the values that are less likely or have a lower chance or lower probability of being seen in the data set.** 


In this example that we have here, it is quite likely to see examples in the ellipse in the middle, and decreases outwards.

![](2024-03-04-22-40-46.png)

And having modeled or having learned to model for $p(x)$, when we are given the new test example $x_{test}$, we compute the probability of $x_{test}$. And if it is small or more precisely, if it is less than some small number epsilon $\epsilon$, which means that $p(x)$ is very small or in other words, the specific value of $x$ that we saw for a certain user was very unlikely, relative to other usage that we have seen. 

But if the $p(x_{test})$ is less than some small threshold or some small number epsilon, we will raise a flag to say that this could be an anomaly. 

So for example, if $x_{test}$ was all the way down and to the right of the graph, the probability of an example landing all the way out here is actually quite low. And so hopefully $p(x_{test})$ for this value of $x_{test}$ will be less than epsilon and so we would flag this as an anomaly.

![](2024-03-04-22-44-39.png)

In contrast, if $p(x_{test})$ is not less than epsilon, that is, if $p(x_{test})$ is greater than equal to epsilon, then we will say that it looks okay, this doesn't look like an anomaly. And that corresponds to an example where our model $p(x)$ will say that examples near the middle are actually quite high probability: there's a very high chance that the new airplane engine will have features close to these inner ellipses.

![](2024-03-04-22-46-06.png)

Anomaly detection is used today in many applications. 

It is frequently used in **fraud detection** where for example: if we are running a website with many different features $x^{(i)}$ (how often does this user login? how many web pages do they visit? How many transactions are they making? how many posts on the discussion forum are they making? what is their typing speed?) With data like this we can then model $p(x)$ from data to model what is the typical behavior of a given user. 

In the common workflow of fraud detection, we wouldn't automatically turn off an account just because it seemed anomalous. Instead we may ask the security team to take a closer look or put in some additional security checks such as ask the user to verify their identity with a cell phone number or ask them to pass a capture to prove that they're human and so on. But algorithms like this are routinely used today to try to find unusual or maybe slightly suspicious activity. So we can more carefully screen those accounts to make sure there isn't something fraudulent. 

![](2024-03-04-22-48-59.png)

Anomaly detection is also frequently used in **manufacturing**. Many manufacturers routinely use anomaly detection to see if whatever they just manufactured. Anything from an airplane engine to a printed circuit board to a smartphone to a motor, to many, many things to see if we've just manufactured the unit that somehow behaves strangely. 

Is also u**sed to monitor computers in clusters and in data centers** where if $x^{(i)}$ are the features of a certain machine (such as memory use, the number of disk accesses per second, CPU load, ratio of CPU load to network traffic) if ever a specific computer behaves very differently than other computers, it might be worth taking a look at that computer to see if something is wrong with it. For example, if it has had a hard disk failure or network card failure or something's wrong with it or if maybe it has been hacked into. 

## Gaussian (normal) distribution

Let's take a look at what is the Gaussian or the normal distribution. 

If $x$ is a random number, sometimes called the random variable, $x$ can take on random values. If the probability of $x$ is given by a Gaussian or normal distribution with mean parameter $\mu$ , and with variance $\sigma^2$, the probability of $x$ looks like a curve that goes like this:

![](2024-03-04-23-01-00.png)

The center or the middle of the curve is given by the mean $\mu$ , and the standard deviation or the width of this curve is given by that variance parameter $\sigma$. Technically, $\sigma$ is called the standard deviation and the square of $\sigma$ is called the variance of the distribution. This curve here shows what is $p(x)$ or the probability of x. 

![](2024-03-04-23-01-55.png)

If we're wondering what does $p(x)$ really means? Here's one way to interpret it: It means that if we were to get, say, 100 numbers drawn from this probability distribution, and we were to plot a histogram of these 100 numbers drawn from this distribution, we might get a histogram that looks like this:

![](2024-03-04-23-02-29.png)

It looks vaguely bell-shaped. What this curve on the left indicates is not if we have just 100 examples or 1,000 or a million or a billion, but if we had a practically infinite number of examples, and we were to draw a histogram of this practically infinite number of examples with a very fine histogram bins. Then we end up with essentially this bell-shaped curve here on the left. 

The formula for $p(x)$ is given by the expression:

$$ $p(x)$ = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$

Let's look at a few examples of how changing $\mu$  and $\sigma$ will affect the Gaussian distribution. First, let me set $\mu$ equals 0 and $\sigma$ equals 1. Here's my plot of the Gaussian distribution with mean 0, $\mu$ equals 0, and standard deviation $\sigma$ equals 1. 

We notice that this distribution is centered at zero and that is the standard deviation $\sigma$ is equal to 1. 

![](2024-03-04-23-06-51.png)

Now, let's reduce the standard deviation $\sigma$ to 0.5. If we plot the Gaussian distribution with $\mu$ equals 0 and $\sigma$ equals 0.5, it now it looks like this.

![](2024-03-04-23-07-09.png)

Notice that it's still centered at zero because $\mu$ is zero. But it's become a much thinner curve because $\sigma$ is now 0.5. We might recall that $\sigma$ is the standard deviation is 0.5, whereas $\sigma$ squared is also called the variance. 

That's equal to 0.5 squared or 0.25. You may have heard that probabilities always have to sum up to one, so that's why the area under the curve is always equal to one, which is why when the Gaussian distribution becomes skinnier, it has to become taller as well. 

![](2024-03-04-23-07-43.png)

Now, we're going to increase $\sigma$ to 2, so the standard deviation is 2 and the variance is 4. This now creates a much wider distribution because $\sigma$ here is now much larger, and because it's now a wider distribution is become shorter as well because the area under the curve is still equals 1.

![](2024-03-04-23-08-13.png)

Finally, let's try changing the mean parameter $\mu$, and I'll leave $\sigma$ equals 0.5. 

![](2024-03-04-23-08-23.png)

In this case, the center of the distribution $\mu$ moves over here to the right. But the width of the distribution is the same as the one on top because the standard deviation is 0.5 in both of these cases on the right. This is how different choices of $\mu$ and $\sigma$ affect the Gaussian distribution. 

When we're applying this to anomaly detection, here's what we have to do: e are given a dataset of $m$ examples, and here $x$ is just a number. Here, are plotted of the training sets with 11 examples:

![](2024-03-04-23-09-03.png)

What we have to do is try to estimate what a good choice is for the mean parameter $\mu$, as well as for the variance parameter $\sigma$ squared. Given a dataset like this, it would seem that a Gaussian distribution maybe looking like that with a center here and a standard deviation like this. This might be a pretty good fit to the data.

![](2024-03-04-23-09-35.png)

The way we would compute $\mu$ and $\sigma$ squared mathematically is our estimate for $\mu$ will be just the average of all the training examples: 

$$ \mu = \frac{1}{m} \sum_{i=1}^m x^{(i)} $$

The value we will use to estimate $\sigma$ squared will be the average of the squared difference between two examples, and that $\mu$ that we just estimated here on the left:

$$ \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu)^2 $$

These formulas for $\mu$ and $\sigma$ squared are technically called **the maximum likelihood estimates for $\mu$ and $\sigma$.** 

If we were to get an example near the tip of the bell, then $p(x)$ is pretty high, whereas if we were to get an example, at the ends of the bell then $p(x)$ is pretty low, which is why we would consider this example, okay, not really anomalous, not a lot like the other ones. 

![](2024-03-04-23-12-52.png)

Now, we've done this only for when $x$ is a number, as if we had only a single feature for our anomaly detection problem. But for practical anomaly detection applications, we will have many features, two or three or some even larger number $n$ of features. Let's take what we saw for a single Gaussian and use it to build a more sophisticated anomaly detection algorithm that can handle multiple features.

## Anomaly detection algorithm

Now that we've seen how the Gaussian or the normal distribution works for a single number, we're ready to build our anomaly detection algorithm. 

Let's dive in. We have a training set $x_1$ through $x_m$, where here each example $x$ has $n$ features. So, each example $x$ is a vector with $n$ numbers. 

In the case of the airplane engine example, we had two features corresponding to the heat and the vibrations. And so, each of these $\vec{\mathbf{x}}^{(i)}$'s would be a two dimensional vector and $n$ would be equal to 2. But for many practical applications $n$ can be much larger and we might do this with dozens or even hundreds of features.

![](2024-03-04-23-24-57.png)

Given this training set, what we would like to do is to carry out density estimation and all that means is, we will build a model or estimate the probability for $p(x)$. What's the probability of any given feature vector? And our model for $p(x)$ is going to be as follows:

$$ p(\vec{\mathbf{x}}) = p(x_1; \mu_1, \sigma_1^2) \space*\space p(x_2; \mu_2, \sigma_2^2) \space*\space p(x_3; \mu_3, \sigma_3^2)\space*\space \cdots \space*\space p(x_n; \mu_n, \sigma_n^2) $$

$x$ is a feature vector with values $x_1$, $x_2$ and so on, down to $x_n$. 

And we're going to model $p(x)$ as the probability of $x_1$, times the probability of $x_2$, times the probability of $x_3$ times the probability of $x_n$, for the $n$ th features in the feature vectors. 

This equation corresponds to assuming that the features $x_1$, $x_2$ and so on up to $x_n$ are statistically independent. But tis algorithm often works fine even that the features are not actually statistically independent. 

Note that we are saying that the probability of all the features of these vector features $x$ each correspond to a different $\mu$ $n$ and $\sigma$.

In case we're wondering why we multiply probabilities: let's see an example for an aircraft engine that has a 1/10 chance that it is really hot, unusually hot. And there is a 1 in 20 chance that it vibrates really hard. Then, what is the chance that it runs really hot and vibrates really hard?

We're saying that the chance of that is 1/10 times 1/20 which is 1/200. So it's really unlikely to get an engine that both run really hot and vibrates really hard. 

![](2024-03-04-23-31-10.png)

A somewhat more compact way to write this equation up here is:

$$ p(\vec{\mathbf{x}}) = \prod_{j=1}^n p(x_j; \mu_j, \sigma _j^2) $$

So let's put it all together to see how we can build an anamoly detection system. 

The first step is to choose features $x^{(i)}$ that we think might be indicative of anomalous examples.

![](2024-03-04-23-33-19.png)

Having come up with the features we want to use, we would then fit the parameters $\mu_1$  through $\mu_n$ and $\sigma_1^2$  1 through \sigma_n^2$, for the $n$ features in our data set. As we might guess, the parameter $\mu_j$ j will be just the average of $x_j$ of the feature $j$ of all the examples in our training set. And $\sigma_j^2$ will be the average of the square difference between the feature and the value $\mu_j$, that we just computed.

![](2024-03-04-23-35-11.png)

And by the way, if we have a vectorized implementation, we can also compute $\mu$ as the average of the training examples as follows: here $x$ and $\mu$ are both vectors. And so this would be the vectorized way of computing $\mu_1$ through $\mu_n$ and all at the same time.

![](2024-03-04-23-36-00.png)

And by estimating these parameters on our unlabeled training set, we've now computed all the parameters of our model. 

Finally, when we are given a new example, $x_{test}$  what we would do is compute $p(x)$ and see if it's large or small. So $p(x)$ as we saw on the last slide is the product from $j = 1$ to $n$ of the probability of the individual features:

$$ p(x) = \prod_{j=1}^n p(x_j; \mu_j, \sigma_j^2) = \prod_{j=1}^n \frac{1}{\sqrt{2\pi}\sigma_j} e^{-\frac{(x_j - \mu_j)^2}{2\sigma_j^2}} $$

![](2024-03-04-23-37-25.png)

And if we compute out this formula, we get some number for $p(x)$. 


And the final step is to see a $p(x)$ is less than epsilon. And if it is then we flag that it is an anomaly.

![](2024-03-04-23-38-16.png)

One intuition behind what this algorithm is doing is that it will tend to flag an example as anomalous if one or more of the features are either very large or very small relative to what it has seen in the training set. 

So for each of the features $x_j$, we're fitting a Gaussian distribution like this. And so if even one of the features of the new example was way out here, say, then $p(x_j)$ would be very small. And if just one of the terms in this product is very small, then this overall product, when we multiply together will tend to be very small and does $p(x)$ will be small.

![](2024-03-04-23-39-46.png)

So, what anomaly detection is doing in this algorithm is a systematic way of quantifying whether or not this new example $x$ has any features that are unusually large or unusually small. 

Now, let's take a look at what all this actually means on one example. Up ahead, a data set with features $x_1$ and $x_2$.

Notice that the features $x_1$ take on a much larger range of values than the features $x_2$. If we were to compute the mean of the Features $x_1$ , we end up with five, which is why we want is equal to 1. And it turns out that for this data said, if we compute $\sigma_1$, it will be equal to about 2. 

And if we were to compute $\mu$ to the average of the features on next to the average is 3 and similarly is variance or standard deviation is much smaller, which is why $\sigma_2$ is equal to 1.

![](2024-03-04-23-42-07.png)

If we were to actually multiply $p(x_1)$  and $p(x_2)$, then we end up with this  3D surface plot for $p(x)$ where any point, the height of this is the product of $p(x_1)$ times $p(x_2)$. 

![](2024-03-04-23-44-04.png)

And this signifies that values where $p(x)$ is higher are more likely. So, values near the middle kind of here are more likely, whereas values far outare much less likely. 

Now, let's pick two test examples, $x^1_{test}$  and $x^2_{test}$ and see what the algorithm is doing; which of these 2 examples the algorithm will flag as anomalous. We're going to pick the parameter Îµ to be equal to 0.02. 

If we were to compute $p(x^1_test)$, it turns out to be about 0.4, which is much bigger than epsilon $\epsilon$. The algorithm will say this looks okay, doesn't look like an anomaly. 

In contrast, if we were to compute $p(x^1_{test})$ for the point corresponding to $x_1$ equals about 8 and $x_2$ equals about 0.5. Then $p(x^2_{test})$ is 0.0021. 

So this is much smaller than epsilon $\epsilon$. And so the algorithm will flag this as a likely anomaly.

## Developing and evaluating an anomaly detection system

Let's see some practical tips for developing an anomaly detection system. The key idea will be that if we can have a way to evaluate a system, even as it's being developed, we'll be able to make decisions and change the system and improve it much more quickly. 

When we are developing a learning algorithm (for example, choosing different features or trying different values of the parameters like $\epsilon$) **making decisions about whether or not to change a feature is much easier if we have a way of evaluating the learning algorithm.** 

![](2024-03-05-17-56-07.png)

This is called **real number evaluation**, meaning that if we can quickly change the algorithm in some way (such as change a feature or change a parameter) and have a way of computing a number that tells us if the algorithm got better or worse, then it makes it much easier to decide whether or not to stick with that change.

This is how it's often done in anomaly detection. 

Rven though we've mainly been talking about unlabeled data, we're going to change that assumption and assume that we have some labeled data, including  a small number usually of previously observed anomalies. (For example, after making airplane engines for a few years, we've just seen a few airplane engines that were anomalous, and we're going to associate those to a label $y = 1$to indicate this anomaly, while normal = 0).

![](2024-03-05-18-06-23.png)

**The training set that the anomaly detection algorithm will learn from is still this unlabeled training set of $x_1$ through $x_m$**, and we're going to think of all of these examples as ones that we'll just assume are normal -not anomalous-, so $y$ is equal to 0. 

![](2024-03-05-18-07-18.png)

In practice, if a few anomalous examples where to slip into this training set, our algorithm will still usually perform okay. 


To evaluate our algorithm, that is, to come up with a way for we to have a real number evaluation, it turns out to be very useful i**f we have a small number of anomalous examples so that we can create a cross validation set**, which we're going to denote $x_{cv}^1$, $y_{cv}^1$ through $x_{cv}^m$, $y_{cv}^m$.


Also, we'd like to have a test set of some number of examples where both the cross validation and test sets hopefully includes a few anomalous examples. In other words, the cross validation and test sets will have a few examples of $y$ equals 1, but also a lot of examples where $y$ is equal to 0.

![](2024-03-05-18-11-02.png)

Let's illustrate this with the aircraft engine example. Let's say we have been manufacturing aircraft engines for years and so we've collected data from 10,000 good or normal engines, but over the years we had also collected data from 20 flawed or anomalous engines:

![](2024-03-05-18-11-28.png)

Usually the number of anomalous engines, that is $y$ equals 1, will be much smaller. It will not be a typical to apply this type of algorithm with anywhere from, say, 2-50 known anomalies. We're going to take this dataset and break it up into a training set, a cross validation set, and the test set:

![](2024-03-05-18-11-51.png)

Here's one example. We're going to put 6,000 good engines into the training set. (Again, if there are couple of anomalous engines that got slipped into this set is actually okay)

Then let's put 2,000 good engines and 10 of the known anomalies into the cross-validation set, and a separate 2,000 good and 10 anomalous engines into the test set. 

![](2024-03-05-18-12-34.png)

What we can do then is **train the algorithm on the training set, fit the Gaussian distributions to these 6,000 examples**.

And then **on the cross-validation set, we can see how many of the anomalous engines it correctly flags.** 

For example, we could use the cross validation set to **tune the parameter epsilon**: set it higher or lower depending on whether the algorithm seems to be reliably detecting these 10 anomalies without taking too many of these 2,000 good engines and flagging them as anomalies. 

![](2024-03-05-18-14-04.png)

After we have tuned the parameter epsilon and maybe **also added or subtracted or tuned features $X_J$** we can then take the algorithm and **evaluate it on our test set to see how many of these 10 anomalous engines it finds, as well as how many mistakes it makes by flagging the good engines as anomalous ones.** 

Notice that this is still primarily an unsupervised learning algorithm because the training sets really has no labels or they all have labels that we're assuming to be $y$ equals 0. So, we learned from the training set by fitting the Gaussian distributions as we saw in the previous section. But it turns out if we're building a practical anomaly detection system, having a small number of anomalies to use to evaluate the algorithm that our cross validation and test sets is very helpful for tuning the algorithm. 

Because the number of flawed engines is so small there's one other alternative used for anomaly detection: **not use a test set, but to have just a training set and a cross-validation set**. 


In this example, we will set train on 6,000 good engines, but take the remainder of the data, the 4,000 remaining good engines as well as all the anomalies, and put them in the cross validation set. We would then tune the parameters $\epsilon$ and add or subtract features $x_j$ to try to get it to do as well as possible as evaluated on the cross validation set. 

![](2024-03-05-18-18-14.png)

If we have very few flawed engines, so if we had only two flawed engines, then this really makes sense to put all of that in the cross validation set. Since we just don't have enough data to create a totally separate test set that is distinct from our cross-validation set. The downside of this alternative here is that after we've tuned our algorithm, we don't have a fair way to tell how well this will actually do on future examples because we don't have the test set. 

Just be aware that there's a higher risk that we will have over-fitted some of our decisions around $\epsilon$ and choice of features and so on, to the cross-validation set, and so its performance on real data in the future may not be as good as we were expecting.

![](2024-03-05-18-19-33.png)

### Algorithm evaluation

**Let's take a closer look at how to actually evaluate the algorithm on our cross-validation sets or on the test set.** Here's what we'd do. 

- We would first fit the model $p(x)$ on the training set. This was a 6,000 examples of goods engines.

![](2024-03-05-18-21-17.png)

- Then on any cross validation or test example $x$, we would compute $p(x)$ and we will predict $y$ equals 1. That is, anomalous if $p(x)$ is less than $\epsilon$ and we predict $y$ is 0, if $p(x)$ is greater than or equal to $\epsilon$. 

![](2024-03-05-18-22-45.png)

- Based on this, we can now look at how accurately this algorithm's predictions on the cross validation or test set matches the labels $y$ that we have in the cross validation or the test sets. 


In the third week of the second course, we had had a couple of optional sections on how to handle highly skewed data distributions where the number of positive examples, $y$ equals 1, can be much smaller than the number of negative examples where $y$ equals 0. This is the case as well for many anomaly detection in the applications where the number of anomalies in our cross-validation set is much smaller. 

In our previous example, we had maybe 10 positive examples and 2,000 negative examples because we had 10 anomalies and 2,000 normal examples. If we saw those optional sections, we may recall that we saw it can be useful to compute things like the true positive, false positive, false negative, and true negative rates. Also compute precision recall or $F_1$ score and that these are alternative metrics and classification accuracy that could work better when our data distribution is very skewed.

![](2024-03-05-18-23-51.png)

So, we might consider applying those types of evaluation metrics as well to tell how well our learning algorithm is doing at finding that small handful of anomalies or positive examples amidst this much larger set of negative examples of normal plane engines.

The intuition I hope we get is to use the cross-validation set to just look at how many anomalies is finding and also how many normal engines is incorrectly flagging as an anomaly. Then to just use that to try to choose a good choice for the parameter $\epsilon$.

![](2024-03-05-18-24-50.png)

## Anomaly detection vs. supervised learning

When we have a few positive examples with $y = 1$ and a large number of negative examples -$y = 0$-, **when should we use anomaly detection and when should we use supervised learning?** 

The decision is actually quite subtle in some applications. So, let's see some suggestions for how to pick between these two types of algorithms. 

**An anomaly detection algorithm will typically be the more appropriate choice when we have a very small number of positive examples (0-20 positive examples is not uncommon) and a relatively large number of negative examples with which to try to build a model for $p(x)$.**

![](2024-03-08-23-55-12.png)

Recall that the parameters for $p(x)$ are learned only from the negative examples and this much smaller set of positive examples is only used in our cross validation set and test set for parameter tuning and for evaluation. 

In contrast, **if we have a larger number of positive and negative examples, then supervised learning might be more applicable.** 

![](2024-03-08-23-56-45.png)

Now, even if we have only 20 positive training examples, it might be okay to apply a supervised learning algorithm. But it turns out that the way anomaly detection looks at the data set versus the way supervised learning looks at the data set are quite different. 

Here is the main difference: **if we think there are many different types of anomalies or many different types of positive examples, then anomaly detection might be more appropriate.**

![](2024-03-08-23-58-20.png)

For example: when there are many different ways for an aircraft engine to go wrong, and if tomorrow there may be a brand new way for an aircraft engine to have something wrong with it, then our 20 positive examples may not cover all of the ways that an aircraft engine could go wrong. **That makes it hard for any algorithm to learn from the small set of positive examples what the anomalies, what the positive examples look like. And future anomalies may look nothing like any of the anomalous examples we've seen so far.** 

If we believe this to be true for our problem, then I would gravitate to using an anomaly detection algorithm. Because what anomaly detection does is it looks at the normal examples that is the $y = 0$ negative examples and just try to model what they look like. And anything that deviates a lot from normal it flags as an anomaly, including if there's a brand new way for an aircraft engine to fail that had never been seen before in our data set.

In contrast, **supervised learning has a different way of looking at the problem: when we're applying supervised learning ideally we would hope to have enough positive examples for the average to get a sense of what the positive examples are like.**

And **with supervised learning, we tend to assume that the future positive examples are likely to be similar to the ones in the training set**.

![](2024-03-09-00-02-52.png)


Let's illustrate this with one example, if we are using a system to find, say financial fraud. There are many different ways unfortunately that some individuals are trying to commit financial fraud. 

And unfortunately there are new types of financial fraud attempts every few months or every year. And what that means is that because they keep on popping up completely new, and unique forms of financial fraud anomaly detection is often used to just look for anything that's different, then transactions we've seen in the past. 

In contrast, if we look at the problem of email spam detection, well, there are many different types of spam email, but even over many years. Spam emails keep on trying to sell similar things or get we to go to similar websites and so on. Spam email that we will get in the next few days is much more likely to be similar to spam emails that we have seen in the past. 

So that's why supervised learning works well for spam because it's trying to detect more of the types of spam emails that we have probably seen in the past in our training set. Whereas if we're trying to detect brand new types of fraud that have never been seen before, then anomaly detection maybe more applicable.

![](2024-03-09-00-03-48.png)

Let's go through a few more examples:

![](2024-03-09-00-05-45.png)


## Choosing what features to use

When building an anomaly detection algorithm, I found that choosing a good choice of features turns out to be really important. In supervised learning, if we don't have the features quite right, or if we have a few extra features that are not relevant to the problem, that often turns out to be okay. 

Because the algorithm has to supervised signal that is enough labels why for the algorithm to figure out what features ignore, or how to re scale feature and to take the best advantage of the features we do give it. But for anomaly detection which runs, or learns just from unlabeled data, is harder for the algorithm to figure out what features to ignore. So I found that carefully choosing the features, is even more important for anomaly detection, than for supervised learning approaches. 

Let's take a look at this section as some practical tips, for how to tune the features for anomaly detection, to try to get we the best possible performance. One step that can help our anomaly detection algorithm, is to try to make sure the features we give it are more or less Gaussian. And if our features are not Gaussian, sometimes we can change it to make it a little bit more Gaussian. 

Let me show we what I mean. If we have a feature X, I will often plot a histogram of the feature which we can do using the python command PLT. Though we see this in the practice lab as well, in order to look at the histogram of the data. 

This distribution here looks pretty Gaussian. So this would be a good candidate feature. If we think this is a feature that helps distinguish between anomalies and normal examples. 

But quite often when we plot a histogram of our features, we may find that the feature has a distribution like this. This does not at all look like that symmetric bell shaped curve. When that is the case, I would consider if we can take this feature X, and transform it in order to make a more Gaussian. 

For example, maybe if we were to compute the log of X and plot a histogram of log of X, look like this, and this looks much more Gaussian. And so if this feature was feature X one, then instead of using the original feature X one which looks like this on the left, we might instead replace that feature with log of X one, to get this distribution over here. Because when X one is made more Gaussian. 

When anomaly detection models P of X one using a Gaussian distribution like that, is more likely to be a good fit to the data. Other than the log function, other things we might do is, given a different feature X two, we may replace it with X two, log of X two plus one. This would be a different way of transforming X two. 

And more generally, log of X two plus C, would be one example of a formula we can use, to change X to try to make it more Gaussian. Or for a different feature, we might try taking the square root or really the square would have executed this X lead to the power of one half,and we may change that exponentially term. So for a different feature X four, we might use X four to the power of one third, for example. 

So when we're building an anomaly detection system, I'll sometimes take a look at my features, and if I see any highly non Gaussian by plotting histogram, I might choose transformations like these or others, In order to try to make it more Gaussian. It turns out a larger value of C, will end up transforming this distribution less. But in practice I just try a bunch of different values of C, and then try to take a look to pick one that looks better in terms of making the distribution more Gaussian. 

Now, let me illustrate how I actually do this and that we put a notebook. So this is what the process of exploring different transformations in the features might look like. When we have a feature X, we can plot a histogram of it as follows. 

It actually looks like there's a pretty cause histogram. Let me increase the number of bins in my history gram to 50. So bins equals 50 there. 

That's what histogram bins. And by the way, if we want to change the color, we can also do so as follows. And if we want to try a different transformation, we can try for example to plot X square root of X. 

So X to the power of 0.5 with again 50 histogram bins, in which case it might look like this. And this actually looks somewhat more Gaussian. But not perfectly, and let's try a different parameter. 

So let me try to the power of 4.25. Maybe I just a little bit too far. It's the old 0.4 that looks pretty Gaussian. 

So one thing we could do is replace X with excellent power of 0.4. And so we would set X to be equal to X to the power of 0.4. And just use the value of X in our training process instead. 

Or let me show we another transformation. Here, we're going to try taking the log of X. So log of X spotted with 50 bins, we're going to use the numpy log function as follows. 

And it turns out we get an error, because it turns out that excellent. This example has some values that are equal to zero, and we'll log of zero is negative infinity is not defined. So common trick is to add just a very tiny number there. 

So exports 0.001, becomes non negative. And so we get the histogram that looks like this. But if we want the distribution to look more Gaussian, we can also play around with this parameter, to try to see if there's a value of that. 

Cause user data to look more symmetric and maybe look more Gaussian as follows. And just as we're doing right now in real time, we can see that, we can very quickly change these parameters and plot the histogram. In order to try to take a look and try to get something a bit more Gaussian, than was the original data next that we saw in this histogram up above. 

If we read the machine learning literature, there are some ways to automatically measure how close these distributions are to Gaussian. But I found it in practice, it doesn't make a big difference, if we just try a few values and pick something that looks right to we, that will work well for all practical purposes. So, by trying things out in Jupiter notebook, we can try to pick a transformation that makes our data more Gaussian. 

And just as a reminder, whatever transformation we apply to the training set, please remember to apply the same transformation to our cross validation and test set data as well. Other than making sure that our data is approximately Gaussian, after we've trained our anomaly detection algorithm, if it doesn't work that well on our cross validation set, we can also carry out an error analysis process for anomaly detection. In other words, we can try to look at where the algorithm is not yet doing well whereas making errors, and then use that to try to come up with improvements. 

So as a reminder, what we want is for P of X to be large. For normal examples X, so greater than equal to epsilon, and p f X to be small or less than epsilon, for the anomalous examples X. When we've learned the model P of X from our unlabeled data, the most common problem that we may run into is that, P of X is comparable in value say is, large for both normal and for anomalous examples. 

As a concrete example, if this is our data set, we might fit that Gaussian into it. And if we have an example in our cross validation set or test set, that is over here, that is anomalous, then this has a pretty high probability. And in fact, it looks quite similar to the other examples in our training set. 

And so, even though this is an anomaly, P of X is actually pretty large. And so the algorithm will fail to flag this particular example as an anomaly. In that case, what I would normally do is, try to look at that example and try to figure out what is it that made me think is an anomaly, even if this feature X one took on values similar to other training examples. 

And if I can identify some new feature say X two, that helps distinguish this example from the normal examples. Then adding that feature, can help improve the performance of the algorithm. Here's a picture showing what I mean. 

If I can come up with a new feature X two, say, we're trying to detect fraudulent behavior, and if X one is the number of transactions they make, maybe this user looks like they're making some of the transactions as everyone else. But if I discover that this user has some insanely fast typing speed, and if I were to add a new feature X two, that is the typing speed of this user. And if it turns out that when I plot this data using the old feature X one and this new feature X two, causes X two to stand out over here. 

Then it becomes much easier for the anomaly detection algorithm to recognize an X two is an anomalous user. Because when we have this new feature X two, the learning algorithm may fit a Gaussian distribution that assigns high probability to points in this region, a bit lower in this region, and a bit lower in this region. And so this example, because of the very anomalous value of X two, becomes easier to detect as an anomaly. 

So just to summarize the development process will often go through is, to train the model and then to see what anomalies in the cross validation set the algorithm is failing to detect. And then to look at those examples to see if that can inspire the creation of new features that would allow the algorithm to spot. That example takes on unusually large or unusually small values on the new features, so that we can now successfully flag those examples as anomalies. 

Just as one more example, let's say we're building an anomaly detection system to monitor computers in the data center. To try to figure out if a computer may be behaving strangely and deserves a closer look, maybe because of a hardware failure, or because it's been hacked into or something. So what we'd like to do is, to choose features that might take on unusually large or small values in the event of an anomaly. 

we might start off with features like X one is the memory use, X two is the number of disk accesses per second, then the CPU load, and the volume of network traffic. And if we train the algorithm, we may find that it detects some anomalies but fails to detect some other anomalies. In that case, it's not unusual to create new features by combining old features. 

So, for example, if we find that there's a computer that is behaving very strangely, but neither is CPU load nor network traffic is that unusual. But what is unusual is, it has a really high CPU load, while having a very low network traffic volume. If we're running the data center that streams sections, then computers may have high CPU load and high network traffic, or low CPU load and no network traffic. 

But what's unusual about this one machine is a very high CPU load, despite a very low traffic volume. In that case, we might create a new feature X five, which is a ratio of CPU load to network traffic. And this new feature with hope, the anomaly detection algorithm flagged future examples like the specific machine we may be seeing as anomalous. 

Or we can also consider other features like the square of the CPU load, divided by the network traffic volume. And we can play around with different choices of these features. In order to try to get it so that P of X is still large for the normal examples but it becomes small in the anomalies in our cross validation set. 

So that's it. Thanks for sticking with me to the end of this week. I hope we enjoy hearing about both clustering algorithms and anomaly detection algorithms. 

And that we also enjoy playing with these ideas in the practice labs. Next week, we'll go on to talk about recommender systems. When we go to a website and recommends products, or movies, or other things to we. 

How does that algorithm actually work? This is one of the most commercially important algorithms in machine learning that gets talked about surprisingly little but next week we'll take a look at how these algorithms work so that we understand the next time we go to the website and then recommend something to we. Maybe how that came about. 

As was we'll be able to build other algorithms like that for yourself as well. So have fun with the labs and they look forward to seeing we next week.