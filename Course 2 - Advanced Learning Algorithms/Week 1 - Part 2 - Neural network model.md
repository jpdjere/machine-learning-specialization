# Neural network model

## Neural network layer

The fundamental building block of most modern neural networks is a layer of neurons. In this video, we'll learn how to construct a layer of neurons and once we have that down, we'd be able to take those building blocks and put them together to form a large neural network. 
Let's take a look at how a layer of neurons works. Here's the example we had from the demand prediction example where we had four input features that were set to this layer of three neurons in the hidden layer that then sends its output to this output layer with just one neuron. Let's zoom in to the hidden layer to look at its computations. 
This hidden layer inputs four numbers and these four numbers are inputs to each of three neurons. Each of these three neurons is just implementing a little logistic regression unit or a little bit logistic regression function. Take this first neuron. 
It has two parameters, w and b. In fact, to denote that, this is the first hidden unit, we're going to subscript this as $w_1$, $b_1$. What it does is I'll output some activation value a, which is g of $w_1$ in a product with x plus $b_1$, where this is the familiar z value that we have learned about in logistic regression in the previous course, and g of z is the familiar logistic function, 1 over 1 plus e to the negative z. 
Maybe this ends up being a number 0.3 and that's the activation value a of the first neuron. To denote that this is the first neuron, we're also going to add a subscript $a_1$ over here, and so $a_1$ may be a number like 0.3. There's a 0.3 chance of this being highly affordable based on the input features. 
Now let's look at the second neuron. The second neuron has parameters $w_2$ and $b_2$, and these w, b or $w_2$, $b_2$ are the parameters of the second logistic unit. It computes $a_2$ equals the logistic function g applied to $w_2$ dot product x plus $b_2$ and this may be some other number, say 0.7. 
Because in this example, there's a 0.7 chance that we think the potential buyers will be aware of this t-shirt. Similarly, the third neuron has a third set of parameters $w_3$, $b_3$. Similarly, it computes an activation value $a_3$ equals g of $w_3$ dot product x plus $b_3$ and that may be say, 0.2. 
In this example, these three neurons output 0.3, 0.7, and 0.2, and this vector of three numbers becomes the vector of activation values a, that is then passed to the final output layer of this neural network. Now, when we build neural networks with multiple layers, it'll be useful to give the layers different numbers. By convention, this layer is called layer 1 of the neural network and this layer is called layer 2 of the neural network. 
The input layer is also sometimes called layer 0 and today, there are neural networks that can have dozens or even hundreds of layers. But in order to introduce notation to help us distinguish between the different layers, we're going to use superscript square bracket 1 to index into different layers. In particular, a superscript in square brackets 1, we're going to use, that's a notation to denote the output of layer 1 of this hidden layer of this neural network, and similarly, $w_1$, $b_1$ here are the parameters of the first unit in layer 1 of the neural network, so we're also going to add a superscript in square brackets 1 here, and $w_2$, $b_2$ are the parameters of the second hidden unit or the second hidden neuron in layer 1. 
Its parameters are also denoted here w^[1] like so. Similarly, I can add superscripts square brackets like so to denote that these are the activation values of the hidden units of layer 1 of this neural network. I know maybe this notation is getting a little bit cluttered. 
But the thing to remember is whenever we see this superscript square bracket 1, that just refers to a quantity that is associated with layer 1 of the neural network. If we see superscript square bracket 2, that refers to a quantity associated with layer 2 of the neural network and similarly for other layers as well, including layer 3, layer 4 and so on for neural networks with more layers. That's the computation of layer 1 of this neural network. 
Its output is this activation vector, a^[1] and we're going to copy this over here because this output $a_1$ becomes the input to layer 2. Now let's zoom into the computation of layer 2 of this neural network, which is also the output layer. The input to layer 2 is the output of layer 1, so $a_1$ is this vector 0.3, 0.7, 0.2 that we just computed on the previous part of this slide. 
Because the output layer has just a single neuron, all it does is it computes $a_1$ that is the output of this first and only neuron, as g, the sigmoid function applied to w $_1$ in a product with a^[1], so this is the input into this layer, and then plus $b_1$. Here, this is the quantity z that we familiar with and g as before is the sigmoid function that we apply to this. If this results in a number, say 0.84, then that becomes the output layer of the neural network. 
In this example, because the output layer has just a single neuron, this output is just a scalar, is a single number rather than a vector of numbers. Sticking with our notational convention from before, we're going to use a superscript in square brackets 2, to denote the quantities associated with layer 2 of this neural network, so a^[2] is the output of this layer, and so we're going to also copy this here as the final output of the neural network. To make the notation consistent, we can also add these superscripts square bracket 2s to denote that these are the parameters and activation values associated with layer 2 of the neural network. 
Once the neural network has computed $a_2$, there's one final optional step that we can choose to implement or not, which is if we want a binary prediction, 1 or 0, is this a top seller? Yes or no? As we can take the number a superscript square brackets 2 subscript 1, and this is the number 0.84 that we computed, and threshold this at 0.5. 
If it's greater than 0.5, we can predict y hat equals 1 and if it is less than 0.5, then predict our y hat equals 0. We saw this thresholding as well when we learned about logistic regression in the first course of the specialization. If we wish, this then gives we the final prediction y hat as either one or zero, if we don't want just the probability of it being a top seller. 
So that's how a neural network works. Every layer inputs a vector of numbers and applies a bunch of logistic regression units to it, and then computes another vector of numbers that then gets passed from layer to layer until we get to the final output layers computation, which is the prediction of the neural network. Then we can either threshold at 0.5 or not to come up with the final prediction. 
With that, let's go on to use this foundation we've built now to look at some even more complex, even larger neural network models. I hope that by seeing more examples, this concept of layers and how to put them together to build a neural network will become even clearer. So let's go on to the next video. 
## More complex neural networks

In the last video, we learned about the neural network layer and how that takes this inputs a vector of numbers and in turn, outputs another vector of numbers. In this video, let's use that layer to build a more complex neural network. Through this, I hope that the notation that we're using for neural networks will become clearer and more concrete as well. 
Let's take a look. This is the running example that we're going to use throughout this video as an example of a more complex neural network. This network has four layers, not counting the input layer, which is also called Layer 0, where layers 1, 2, and 3 are hidden layers, and Layer 4 is the output layer, and Layer 0, as usual, is the input layer. 
By convention, when we say that a neural network has four layers, that includes all the hidden layers in the output layer, but we don't count the input layer. This is a neural network with four layers in the conventional way of counting layers in the network. Let's zoom in to Layer 3, which is the third and final hidden layer to look at the computations of that layer. 
Layer 3 inputs a vector, a superscript square bracket 2 that was computed by the previous layer, and it outputs $a_3$, which is another vector. What is the computation that Layer 3 does in order to go from $a_2$ to $a_3$? If it has three neurons or we call it three hidden units, then it has parameters $w_1$, $b_1$, $w_2$, $b_2$, and $w_3$, $b_3$ and it computes $a_1$ equals sigmoid of $w_1$. 
product with this input to the layer plus $b_1$, and it computes $a_2$ equals sigmoid of $w_2$. product with again $a_2$, the input to the layer plus $b_2$ and so on to get $a_3$. Then the output of this layer is a vector comprising $a_1$, $a_2$, and $a_3$. 
Again, by convention, if we want to more explicitly denote that all of these are quantities associated with Layer 3 then we add in all of these superscript, square brackets 3 here, to denote that these parameters w and b are the parameters associated with neurons in Layer 3 and that these activations are activations with Layer 3. Notice that this term here is $w_1$ superscript square bracket 3, meaning the parameters associated with Layer 3. product with a superscript square bracket 2, which was the output of Layer 2, which became the input to Layer 3. 
That's why it has $a_3$ here because it's a parameter associator of Layer 3. product with, and there's $a_2$ there because is the output of Layer 2. Now, let's just do a quick double check on our understanding of this. 
we're going to hide the superscripts and subscripts associated with the second neuron and without rewinding this video, go ahead and rewind if we want, but prefer we not. But without rewinding this video, are we able to think through what are the missing superscripts and subscripts in this equation and fill them in yourself? Once we take a look at the end video quiz and see if we can figure out what are the appropriate superscripts and subscripts for this equation over here. 
If we chose the 1st option, then we got it right! The activation of the 2nd neuron at layer 3 is denoted by 'a' three two. To apply the activation function, g, lets use the parameters of this same neuron. 
So w and b will have the same subscript 2 and superscript square bracket 3. The input features will be the output vector from the previous layer, which is layer 2. So that will be the vector 'a' superscript 2. 
The second option is using vector ‘a’ 3 which is not the output vector from the previous layer. The input to this layer is 'a' two. And the 3rd option has a two two as input, which is a single number rather than the vector Because recall that the correct input is a vector, a two, with the little arrow on top, and not a single number. 
To recap, $a_3$ is activation associated with Layer 3 for the second neuron hence, this $a_2$ is a parameter associated with the third layer. For the second neuron, this is $a_2$, same as above and then plus $b_3$ too. Hopefully, that makes sense. 
Just the more general form of this equation for an arbitrary Layer 0 and for an arbitrary unit j, which is that a deactivation outputs of layer l, unit j, like a32, that's going to be the sigmoid function applied to this term, which is the wave vector of layer l, such as Layer 3 for the jth unit so there's $a_2$ again, in the example above, and so that's dot-producted with a deactivation value. Notice, this is not l, this is l minus 1, like $a_2$ above here because we're dot-producting with the output from the previous layer and then plus b, the parameter for this layer for that unit j. This gives we the activation of layer l unit j, where the superscript in square brackets l denotes layer l and a subscript j denotes unit j. 
When building neural networks, unit j refers to the jth neuron, so we use those terms a little bit interchangeably where each unit is a single neuron in the layer. G here is the sigmoid function. In the context of a neural network, g has another name, which is also called the activation function, because g outputs this activation value. 
When I say activation function, I mean this function g here. So far, the only activation function we've seen, this is a sigmoid function but next week, we'll look at when other functions, then the sigmoid function can be plugged in place of g as well.. The activation function is just that function that outputs these activation values. 
Just one last piece of notation. In order to make all this notation consistent, we're also going to give the input vector X and another name which is $a_0$, so this way, the same equation also works for the first layer, where when l is equal to 1, the activations of the first layer, that is $a_1$, would be the sigmoid times the weights dot-product with $a_0$, which is just this input feature vector X. With this notation, we now know how to compute the activation values of any layer in a neural network as a function of the parameters as well as the activations of the previous layer. 
we now know how to compute the activations of any layer given the activations of the previous layer. Let's put this into an inference algorithm for a neural network. In other words, how to get a neural network to make predictions. 
Let's go see that in the next video. ## Inference: making predictions (forward propagation)

Let's take what we've learned and put it together into an algorithm to let our neural network make inferences or make predictions. This will be an algorithm called forward propagation. 
Let's take a look. we're going to use as a motivating example, handwritten digit recognition. And for simplicity we are just going to distinguish between the handwritten digits zero and one. 
So it's just a binary classification problem where we're going to input an image and classify, is this the digit zero or the digit one? And we get to play with this yourself later this week in the practice lab as well. For the example of the slide, we're going to use an eight by eight image. 
And so this image of a one is this grid or matrix of eight by eight or 64 pixel intensity values where 255 denotes a bright white pixel and zero would denote a black pixel. And different numbers are different shades of gray in between the shades of black and white. Given these 64 input features, we're going to use the neural network with two hidden layers. 
Where the first hidden layer has 25 neurons or 25 units. Second hidden layer has 15 neurons or 15 units. And then finally the output layer or outputs unit, what's the chance of this being 1 versus 0?. 
So let's step through the sequence of computations that in our neural network will need to make to go from the input X, this eight by eight or 64 numbers to the predicted probability a3. The first computation is to go from X to a1, and that's what the first layer of the first hidden layer does. It carries out a computation of a super strip square bracket 1 equals this formula on the right. 
Notice that a one has 25 numbers because this hidden layer has 25 units. Which is why the parameters go from w1 through w25 as well as b1 through b25. And I've written x here but I could also have written a0 here because by convention the activation of layer zero, that is a0 is equal to the input feature value x. 
So let's just compute a1. The next step is to compute a2. Looking at the second hidden layer, it then carries out this computation where a2 is a function of a1 and it's computed as the safe point activation function applied to w dot product a1 plus the corresponding value of b. 
Notice that layer two has 15 neurons or 15 units, which is why the parameters Here run from w1 through w15 and b1 through b15. Now we've computed a2. The Final step is then to compute a3 and we do so using a very similar computation. 
Only now, this third layer, the output layer has just one unit, which is why there's just one output here. So a3 is just a scalar. And finally we can optionally take a3 subscript one and threshold it at 4.5 to come up with a binary classification label. 
Is this the digit 1? Yes or no? So the sequence of computations first takes x and then computes a1, and then computes a2, and then computes a3, which is also the output of the neural networks. 
we can also write that as f(x). So remember when we learned about linear regression and logistic regression, we use f(x) to denote the output of linear regression or logistic regression. So we can also use f(x) to denote the function computed by the neural network as a function of x. 
Because this computation goes from left to right, we start from x and compute a1, then a2, then a3. This album is also called forward propagation because we're propagating the activations of the neurons. So we're making these computations in the forward direction from left to right. 
And this is in contrast to a different algorithm called backward propagation or back propagation, which is used for learning. And that's something we learn about next week. And by the way, this type of neural network architecture where we have more hidden units initially and then the number of hidden units decreases as we get closer to the output layer. 
There's also a pretty typical choice when choosing neural network architectures. And we see more examples of this in the practice lab as well. So that's neural network inference using the forward propagation algorithm. 
And with this, we'd be able to download the parameters of a neural network that someone else had trained and posted on the Internet. And we'd be able to carry out inference on our new data using their neural network. Now that we've seen the math and the algorithm, let's take a look at how we can actually implement this in tensorflow. 
Specifically, let's take a look at this in the next video.